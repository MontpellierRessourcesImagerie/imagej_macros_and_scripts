{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781ae1d6-26ee-4148-bd47-0760e02bb0e0",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772fd73-fa53-4f86-8c3f-e15d804aa254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d73fd1-4b49-475d-b0e1-0db2a6495a5f",
   "metadata": {},
   "source": [
    "Set the input paths and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685088a6-5b75-4f32-811a-c4b1329f8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_path = \"/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/train/image\"\n",
    "train_mask_path = \"/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/train/mask\"\n",
    "test_input_path = \"/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/test/image\"\n",
    "test_mask_path = \"/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/test/mask\"\n",
    "model_path = \"/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056aed3b-a142-4431-9a67-330458b1f44b",
   "metadata": {},
   "source": [
    "Get the paths of the images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9d80e-2ab1-4b2a-9d07-206faafa2daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_paths = [os.path.join(train_input_path, path) for path in os.listdir(train_input_path) if path.endswith(\".tif\")]\n",
    "train_mask_paths = [os.path.join(train_mask_path, path) for path in os.listdir(train_mask_path) if path.endswith(\".tif\")]\n",
    "print(\"Input images: \" + str(len(train_input_paths)))\n",
    "print(\"Input masks: \" + str(len(train_mask_paths)))\n",
    "print(\"---\")\n",
    "test_input_paths = [os.path.join(test_input_path, path) for path in os.listdir(test_input_path) if path.endswith(\".tif\")]\n",
    "test_mask_paths = [os.path.join(test_mask_path, path) for path in os.listdir(test_mask_path) if path.endswith(\".tif\")]\n",
    "print(\"Test images: \" + str(len(test_input_paths)))\n",
    "print(\"Test masks: \" + str(len(test_mask_paths)))\n",
    "train_path_dataset = tf.data.Dataset.from_tensor_slices((train_input_paths, train_mask_paths))\n",
    "test_path_dataset = tf.data.Dataset.from_tensor_slices((test_input_paths, test_mask_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642c138-51c5-4125-890a-09ebf07c3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in train_path_dataset.take(1):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c973b4-f9c4-42ba-bf76-aa7bed28d317",
   "metadata": {},
   "source": [
    "We define a function to read image/mask pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c6414-bbdf-4491-bf52-c38b2f412209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(img_path, segmentation_mask_path):\n",
    "    img_data = tf.io.read_file(img_path)\n",
    "    img = tfio.experimental.image.decode_tiff(img_data)\n",
    "    img = img[:,:,0:3]\n",
    "    segm_data = tf.io.read_file(segmentation_mask_path)\n",
    "    segm_mask = tfio.experimental.image.decode_tiff(segm_data)   \n",
    "    segm_mask = segm_mask[:,:,0:1]\n",
    "    return img, segm_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2ecf9-1ddc-4f43-8283-88fab918601b",
   "metadata": {},
   "source": [
    "Normalize images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cccf0ff-0bb1-4287-b274-a9f6cc092da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(img, semg_mask):\n",
    "    # img = tfio.experimental.color.rgba_to_rgb(img)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    semg_mask = tf.image.convert_image_dtype(semg_mask, tf.float32)\n",
    "    semg_mask = semg_mask / 255.0\n",
    "    return img, semg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e97685-325c-44d9-bce9-1046ce06fbbb",
   "metadata": {},
   "source": [
    "We create a dataset containing pairs of images/masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887e5ed-1eec-4cd9-ae24-7366da4619d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_path_dataset.map(read_images, num_parallel_calls=tf.data.AUTOTUNE).map(prepare_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_path_dataset.map(read_images, num_parallel_calls=tf.data.AUTOTUNE).map(prepare_images, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6a076-2650-4f6e-86e0-5f1f2101e618",
   "metadata": {},
   "source": [
    "Build train and validation batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c834af-7965-4cc7-8f31-03d3e065a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 1000\n",
    "VALIDATION_SIZE = int(round((len(train_dataset) * 20) / 100))\n",
    "print(\"validation data size: \" + str(VALIDATION_SIZE))\n",
    "print(\"train data size: \" + str(len(train_dataset) - VALIDATION_SIZE))\n",
    "validation_batches = train_dataset.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
    "train_batches = train_dataset.skip(VALIDATION_SIZE)\n",
    "train_batches = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_batches = train_batches.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59803a18-3155-454e-b020-173f9668301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf091cf-0759-4551-ba5c-16a27894fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dataset.take(1)\n",
    "mask = data.get_single_element()[1]\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad3b9a-6f15-436c-a8ad-707fc1689cfe",
   "metadata": {},
   "source": [
    "Display some random examples of pairs of input tiles and mask tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4e296-a1df-4640-a43b-f17c15da8457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "N = 3\n",
    "for image, mask in train_dataset.shuffle(len(train_dataset)).take(N):\n",
    "    print(image.shape)\n",
    "    print(mask.shape)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(image)\n",
    "    ax2.imshow(mask)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ee497-b804-4471-9a34-ad109ea324af",
   "metadata": {},
   "source": [
    "Building blocks for the UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9bd22-5d7b-40d2-9890-ddbd79fdef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_block(x, n_filters):\n",
    "   # Conv2D then ReLU activation\n",
    "   x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "   # Conv2D then ReLU activation\n",
    "   x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "   return x\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "   f = double_conv_block(x, n_filters)\n",
    "   p = layers.MaxPool2D(2)(f)\n",
    "   p = layers.Dropout(0.3)(p)\n",
    "   return f, p\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "   # upsample\n",
    "   x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
    "   # concatenate\n",
    "   x = layers.concatenate([x, conv_features])\n",
    "   # dropout\n",
    "   x = layers.Dropout(0.3)(x)\n",
    "   # Conv2D twice with ReLU activation\n",
    "   x = double_conv_block(x, n_filters)\n",
    "   return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07788968-fff5-4c06-bce2-6f6281cc917c",
   "metadata": {},
   "source": [
    "Function that builds the UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3bb9e-6e68-429f-adb4-fe3897f9517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_model():\n",
    "   inputs = layers.Input(shape=(256,256,3))\n",
    "   # encoder: contracting path - downsample\n",
    "   # 1 - downsample\n",
    "   f1, p1 = downsample_block(inputs, 64)\n",
    "   # 2 - downsample\n",
    "   f2, p2 = downsample_block(p1, 128)\n",
    "   # 3 - downsample\n",
    "   f3, p3 = downsample_block(p2, 256)\n",
    "   # 4 - downsample\n",
    "   f4, p4 = downsample_block(p3, 512)\n",
    "   # 5 - bottleneck\n",
    "   bottleneck = double_conv_block(p4, 1024)\n",
    "   # decoder: expanding path - upsample\n",
    "   # 6 - upsample\n",
    "   u6 = upsample_block(bottleneck, f4, 512)\n",
    "   # 7 - upsample\n",
    "   u7 = upsample_block(u6, f3, 256)\n",
    "   # 8 - upsample\n",
    "   u8 = upsample_block(u7, f2, 128)\n",
    "   # 9 - upsample\n",
    "   u9 = upsample_block(u8, f1, 64)\n",
    "   # outputs\n",
    "   outputs = layers.Conv2D(1, (1,1), padding=\"same\", activation = \"sigmoid\")(u9)\n",
    "   # unet model with Keras Functional API\n",
    "   unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "   return unet_model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0225aac-60da-468c-b5c0-b44dc3db466a",
   "metadata": {},
   "source": [
    "Build the UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dac285-df6d-4579-ac9b-6df1114aa0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = build_unet_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1f808-0560-40e8-9624-91ea051aa5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet_collection import models\n",
    "unet_model = models.unet_2d((None, None, 3), [64, 128, 256, 512, 1024], n_labels=2,\n",
    "                      stack_num_down=2, stack_num_up=1,\n",
    "                      activation='GELU', output_activation='Softmax', \n",
    "                      batch_norm=True, pool='max', unpool='nearest', name='unet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc47808-a676-4aba-82ff-02a5d0ef6920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.keras.utils.register_keras_serializable\n",
    "def dice_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return 1 - (2. * intersection + 1) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + 1)\n",
    "\n",
    "#@tf.keras.utils.register_keras_serializable\n",
    "def bce_dice_loss(bce_coef=0.5):\n",
    "    def bcl(y_true, y_pred):\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "        dice = dice_loss(y_true, y_pred)\n",
    "        return bce_coef * bce + (1.0 - bce_coef) * dice\n",
    "    return bcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32511d1-36df-45dd-a256-30d4d8813826",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(unet_model, show_shapes=True)\n",
    "\"model.png written\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983f8cb-6796-41af-bbbc-69332629a658",
   "metadata": {},
   "source": [
    "Compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a5cfc-accb-4dd4-a5b4-5ae111f1aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=bce_dice_loss(bce_coef=0.3),\n",
    "                  metrics=[tf.keras.metrics.Precision(),\n",
    "                           tf.keras.metrics.Recall(),\n",
    "                           tf.keras.metrics.Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c214323a-a33b-479e-a303-024e8b91d974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "keras.config.disable_traceback_filtering()\n",
    "STEPS_PER_EPOCH = len(train_dataset) // BATCH_SIZE\n",
    "VAL_SUBSPLITS = 5\n",
    "VAL_LENGTH = VALIDATION_SIZE\n",
    "VALIDATION_STEPS = VAL_LENGTH // BATCH_SIZE // VAL_SUBSPLITS\n",
    "model_history = unet_model.fit(train_batches,\n",
    "                              epochs=NUM_EPOCHS,\n",
    "                              steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                              validation_steps=VALIDATION_STEPS,\n",
    "                              validation_data=validation_batches,\n",
    "                              verbose=2\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a02604-675d-429a-b94b-56a460a0d64c",
   "metadata": {},
   "source": [
    "Save a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c49187-4ed2-4013-bf96-0baf9cbed6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.now()\n",
    "unet_model.save(os.path.join(model_path, \"./unet - \" + str(date) + \".keras\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6178d7-eba5-4048-af43-41e5b9d04743",
   "metadata": {},
   "source": [
    "Save the weights only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed678259-e220-4b99-90a9-cabfd79cb2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.now()\n",
    "unet_model.save_weights(os.path.join(model_path, \"./unet - \" + str(date) + \".weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892557e-4195-4903-b53a-5616903f7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2844fa-ee88-422a-acb6-c252c34e1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(model_history.history['accuracy'])\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f71efe-75ad-448c-a345-7245ba0732b8",
   "metadata": {},
   "source": [
    "Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4aa49b-9e30-47d0-9c91-3128bdc2c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"unet - 2025-01-23 11:09:24.145873.keras\"\n",
    "path = os.path.join(model_path, model)\n",
    "unet_model = keras.models.load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dfc809-2206-470d-8dbe-53e0609933b3",
   "metadata": {},
   "source": [
    "Load the weights only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ddfee-5aa6-411b-bf91-12fc1bf98d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'unet - 2025-01-29 11:27:03.366954.weights.h5'\n",
    "path = os.path.join(model_path, model)\n",
    "unet_model.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db17b8a-8826-4732-99b5-04c8943699ba",
   "metadata": {},
   "source": [
    "Evaluate model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bdd3f-27c7-43db-bc5f-2c0cbe35a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = train_dataset.take(len(test_dataset)).batch(BATCH_SIZE)\n",
    "score = unet_model.evaluate(test_batches, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0b798-801f-4900-9b49-47461df025d0",
   "metadata": {},
   "source": [
    "# Apply the model to an image.\n",
    "\n",
    "Functions to read and prepare single images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b98491-2a2a-4989-8380-4860e23971e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(img_path):\n",
    "    img_data = tf.io.read_file(img_path)\n",
    "    img = tfio.experimental.image.decode_tiff(img_data)\n",
    "    img = img[:,:,0:3]\n",
    "    return img\n",
    "\n",
    "def prepare_image(img):   \n",
    "    img = tf.image.convert_image_dtype(img, \"float32\") # This also scales to [O,1)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf9fede-c9d7-4209-a55e-b6ad9e1bcd82",
   "metadata": {},
   "source": [
    "Create a dataset with patches of one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f922ae15-7eaf-4cd2-b3ad-bcd486214c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "image_file = '/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/4 E 1 m_8µm_x50.tif'\n",
    "output_file = '/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/out/4 E 1 m_8µm_x50.tif'\n",
    "image_path_dataset = tf.data.Dataset.from_tensor_slices([image_file])\n",
    "image_dataset = image_path_dataset.map(read_image, num_parallel_calls=tf.data.AUTOTUNE).map(prepare_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "ksize_rows = 256\n",
    "ksize_cols = 256\n",
    "strides_rows = 196\n",
    "strides_cols = 196\n",
    "\n",
    "\n",
    "image = ds # tfio.experimental.image.decode_tiff(image_data)\n",
    "\n",
    "print(image)\n",
    "# The size of sliding window\n",
    "ksizes = [1, ksize_rows, ksize_cols, 1] \n",
    "\n",
    "# How far the centers of 2 consecutive patches are in the image\n",
    "strides = [1, strides_rows, strides_cols, 1]\n",
    "\n",
    "# The document is unclear. However, an intuitive example posted on StackOverflow illustrate its behaviour clearly. \n",
    "# http://stackoverflow.com/questions/40731433/understanding-tf-extract-image-patches-for-extracting-patches-from-an-image\n",
    "rates = [1, 1, 1, 1] # sample pixel consecutively\n",
    "\n",
    "# padding algorithm to used\n",
    "padding='SAME' # or 'SAME'\n",
    "\n",
    "# image = tf.expand_dims(image, 0)\n",
    "image_patches = tf.image.extract_patches(images=list(image_dataset.take(1)), sizes=ksizes, strides=strides, rates=rates, padding=padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d5fea-1302-4d51-ab83-ed29b5c642b2",
   "metadata": {},
   "source": [
    "Display the input patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4cc213-6320-4699-a974-cae8faabcd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = image_patches.shape[1]\n",
    "rows = image_patches.shape[2]\n",
    "\n",
    "print(columns, rows)\n",
    "# retrieve the 1st patches\n",
    "fig = plt.figure(figsize=(columns, rows)) \n",
    "fig.tight_layout()\n",
    "i = 1\n",
    "for col in range(columns):\n",
    "    for row in range(rows):\n",
    "        patch = image_patches[0,col,row,]\n",
    "        patch = tf.reshape(patch, [ksize_rows, ksize_cols, 3])        \n",
    "        fig.add_subplot(columns, rows, i) \n",
    "        plt.axis('off') \n",
    "        plt.imshow(patch)\n",
    "        i = i + 1\n",
    "# visualize image\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657fe53b-59a5-4802-a6ed-5493c1052171",
   "metadata": {},
   "source": [
    "Predict the rings on the patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb721d5-ff64-4167-b9b7-99058d472ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = image_patches.shape[1]\n",
    "rows = image_patches.shape[2]\n",
    "\n",
    "print(columns, rows)\n",
    "# retrieve the 1st patches\n",
    "fig = plt.figure(figsize=(columns, rows)) \n",
    "fig.tight_layout()\n",
    "i = 1\n",
    "results = []\n",
    "for col in range(columns):\n",
    "    for row in range(rows):\n",
    "        patch = image_patches[0,col,row,]\n",
    "        patch = tf.reshape(patch, [1, ksize_rows, ksize_cols, 3])        \n",
    "        res = unet_model.predict([patch], verbose=0)\n",
    "        res = np.squeeze(res)\n",
    "        results.append(res)\n",
    "        fig.add_subplot(columns, rows, i) \n",
    "        plt.axis('off') \n",
    "        strides_rows\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d56c1f-fdd5-4574-afee-4f70c426fecf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ddc005-d169-4c83-930e-d2a7a30c5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(columns, rows)) \n",
    "output = np.array(results).reshape(columns, rows, ksize_cols, ksize_rows)\n",
    "img = output[11][16]\n",
    "img = (img > 0.000005).astype('uint8')\n",
    "plt.imshow(img)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e5b24-b3b8-4260-a563-9d04464f0b5f",
   "metadata": {},
   "source": [
    "Batch apply classifier\n",
    "\n",
    "Set the input and output folders and the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81cc58b-dd01-45e7-a0a0-b921ff8ae34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/input/\"\n",
    "OUTPUT_FOLDER = \"/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/out/\"\n",
    "PATCH_SIZE = 256\n",
    "STRIDE_WIDTH = 256\n",
    "CHANNELS = 3\n",
    "PADDING = 'VALID'\n",
    "THRESHOLD = 0.000005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64971a-ae5d-45d0-9ced-be73486427b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "output = np.array(results).reshape(columns, rows, ksize_cols, ksize_rows)\n",
    "height = list(image_dataset.take(1))[0].shape[0]\n",
    "width = list(image_dataset.take(1))[0].shape[1] \n",
    "reconstructed = np.zeros(height*width).reshape(height, width)\n",
    "print(\"image shape\", reconstructed.shape)\n",
    "y = 0\n",
    "i = 0\n",
    "for col in range(columns):\n",
    "    x = 0\n",
    "    for row in range(rows):   \n",
    "        print(\"row: \", row, \" col: \", col)\n",
    "        yEnd = min(y+ksize_rows, height)\n",
    "        xEnd = min(x+ksize_cols, width)\n",
    "        deltaY = yEnd - y \n",
    "        deltaX = xEnd - x \n",
    "        reconstructed[y:yEnd, x:xEnd] = output[col, row, 0:deltaY, 0:deltaX]\n",
    "        x = x + strides_cols \n",
    "    y = y + strides_rows\n",
    "reconstructed = ((reconstructed > 0.000005).astype('uint8'))*255\n",
    "reconstructed = np.roll(reconstructed, -(width % strides_cols), axis=0)\n",
    "reconstructed = np.roll(reconstructed, -(height % strides_rows), axis=1)\n",
    "cv2.imwrite(output_file, reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba55483-9fc4-450f-ab18-1d49fad47e71",
   "metadata": {},
   "source": [
    "Function to create patches, predict result patches from input patches and reconstruct a result image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f73326-affa-4d00-bab1-5e5a9285bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPatches(image_file, patch_size=255, stride_width=196, padding='SAME'):\n",
    "    image_path_dataset = tf.data.Dataset.from_tensor_slices([image_file])\n",
    "    image_dataset = image_path_dataset.map(read_image, num_parallel_calls=tf.data.AUTOTUNE).map(prepare_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    height = list(image_dataset.take(1))[0].shape[0]\n",
    "    width = list(image_dataset.take(1))[0].shape[1] \n",
    "    ksizes = [1, patch_size, patch_size, 1] \n",
    "    strides = [1, stride_width, stride_width, 1]\n",
    "    rates = [1, 1, 1, 1]\n",
    "    patches = tf.image.extract_patches(images=list(image_dataset.take(1)), sizes=ksizes, strides=strides, rates=rates, padding=padding)\n",
    "    return patches, height, width\n",
    "\n",
    "def predictPatches(model, image_patches, patch_size=256, channels=3):\n",
    "    columns = image_patches.shape[1]\n",
    "    rows = image_patches.shape[2]\n",
    "    results = []\n",
    "    for col in range(columns):\n",
    "        for row in range(rows):\n",
    "            patch = image_patches[0,col,row,]\n",
    "            patch = tf.reshape(patch, [1, patch_size, patch_size, channels])        \n",
    "            res = model.predict([patch], verbose=0)\n",
    "            res = np.squeeze(res)\n",
    "            results.append(res)\n",
    "    output = np.array(results).reshape(columns, rows, patch_size, patch_size)            \n",
    "    return output\n",
    "\n",
    "def reconstructFromPatches(patches, original_image_height, original_image_width, patch_size=256, stride_width=196, threshold=0.000005):\n",
    "    height, width = original_image_height, original_image_width\n",
    "    rows = patches.shape[1]\n",
    "    columns = patches.shape[0]\n",
    "    reconstructed = np.zeros(height*width).reshape(height, width)    \n",
    "    y = 0\n",
    "    for col in range(columns):\n",
    "        x = 0\n",
    "        for row in range(rows):   \n",
    "            yEnd = min(y+patch_size, height)\n",
    "            xEnd = min(x+patch_size, width)\n",
    "            deltaY = yEnd - y \n",
    "            deltaX = xEnd - x\n",
    "            reconstructed[y:yEnd, x:xEnd] = patches[col, row, 0:deltaY, 0:deltaX]\n",
    "            x = x + stride_width \n",
    "        y = y + stride_width\n",
    "    reconstructed = ((reconstructed > threshold).astype('uint8')) * 255\n",
    "    return reconstructed\n",
    "\n",
    "\n",
    "def displayPatches(patches, patch_size=256, channels=3):\n",
    "    if tf.is_tensor(patches):\n",
    "        columns = patches.shape[1]\n",
    "        rows = patches.shape[2]\n",
    "    else:\n",
    "        columns = patches.shape[0]\n",
    "        rows = patches.shape[1]\n",
    "    fig = plt.figure(figsize=(columns, rows)) \n",
    "    fig.tight_layout()\n",
    "    i = 1\n",
    "    for col in range(columns):\n",
    "        for row in range(rows):\n",
    "            if tf.is_tensor(patches):\n",
    "                patch = patches[0, col, row,]\n",
    "                patch = tf.reshape(patch, [patch_size, patch_size, channels])        \n",
    "            else:\n",
    "                patch = patches[col, row]\n",
    "            fig.add_subplot(columns, rows, i) \n",
    "            plt.axis('off') \n",
    "            plt.imshow(patch)\n",
    "            i = i + 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a285b88-c4a6-4861-bbe0-ac259f0d2d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def predictImage(image_file, model, patch_size=256, stride_width=196, padding=\"SAME\", channels=3, threshold= 0.000005):\n",
    "    imagePatches, height, width = createPatches(image_file, patch_size, stride_width, padding)\n",
    "    maskPatches = predictPatches(model, imagePatches, patch_size, channels)\n",
    "    mask = reconstructFromPatches(maskPatches, height, width, patch_size, stride_width, threshold)\n",
    "    return mask\n",
    "\n",
    "def batchPredict(input_folder, output_folder, patch_size=256, stride_width=196, padding=\"SAME\", channels=3, threshold= 0.000005):\n",
    "    predict_input_paths = [os.path.join(input_folder, path) for path in os.listdir(input_folder) if path.endswith(\".tif\")]\n",
    "    predict_output_paths = [os.path.join(output_folder, path) for path in os.listdir(input_folder) if path.endswith(\".tif\")]\n",
    "    paths = zip(predict_input_paths, predict_output_paths)\n",
    "    counter = 1\n",
    "    for input_file, output_file in paths:\n",
    "        print(\"Processing image \" + str(counter) + \" of \" + str(len(predict_input_paths)))\n",
    "        print(\"in: \", input_file)\n",
    "        print(\"out: \", output_file)\n",
    "        mask = predictImage(input_file, unet_model, patch_size, stride_width, padding, channels, threshold)\n",
    "        cv2.imwrite(output_file, mask)\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73926a6-37f9-4f49-aecf-aa2b36328a21",
   "metadata": {},
   "source": [
    "Test create patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a48648-fd50-4b42-a9cf-141c7389eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/input/T 5 b_8µm_x50.tif\"\n",
    "imagePatches, height, width = createPatches(IMAGE_PATH, PATCH_SIZE, STRIDE_WIDTH, PADDING)\n",
    "print(height, width)\n",
    "displayPatches(imagePatches, PATCH_SIZE, CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d08109-d66e-4e2f-b67f-04a919165ac4",
   "metadata": {},
   "source": [
    "Test predict patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e45cf-6d7a-40e1-af02-6d2975f9334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predictPatches(unet_model, imagePatches, PATCH_SIZE, CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcb3f7d-f4ba-470e-a47d-b7d6c9057773",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b175cb44-e806-4266-96a1-ea7a5c078766",
   "metadata": {},
   "outputs": [],
   "source": [
    "displayPatches(output, PATCH_SIZE, CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b96977-55aa-4463-a468-1d017c7ffbe7",
   "metadata": {},
   "source": [
    "Test reconstruct from patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce2723-6d48-49a1-98ef-26dd7aa5da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = reconstructFromPatches(output, height, width, PATCH_SIZE, STRIDE_WIDTH, THRESHOLD)\n",
    "plt.figure(figsize=(8, 8)) \n",
    "plt.axis('off') \n",
    "plt.imshow(reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb429ec1-e1b8-4318-9ebb-6e62eec853be",
   "metadata": {},
   "source": [
    "Test predict image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3586fa3-d96f-4c97-b964-dfec7c571d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = predictImage('/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/input/4 E 1 b_8µm_x50.tif', unet_model, 256, 256, \"VALID\", 3, 0.000005)\n",
    "plt.figure(figsize=(8, 8)) \n",
    "plt.axis('off') \n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4783c-3118-4b8e-92b3-933de6f3eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(output), output.shape, tf.is_tensor(output))\n",
    "print(type(imagePatches), imagePatches.shape, tf.is_tensor(imagePatches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9304e9-d18b-4deb-86b6-295324cf8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchPredict('/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/unused', '/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/unused/out', PATCH_SIZE, STRIDE_WIDTH, PADDING, CHANNELS, THRESHOLD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
