{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781ae1d6-26ee-4148-bd47-0760e02bb0e0",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772fd73-fa53-4f86-8c3f-e15d804aa254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d73fd1-4b49-475d-b0e1-0db2a6495a5f",
   "metadata": {},
   "source": [
    "Set the input paths and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685088a6-5b75-4f32-811a-c4b1329f8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_path = \"/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/train/image\"\n",
    "train_mask_path = \"/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/train/mask\"\n",
    "test_input_path = \"/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/test/image\"\n",
    "test_mask_path = \"/media/baecker/6b38a953-6650-4da5-94d9-57bd718df733/2025/in/2007_tree_rings/input_images/test/mask\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056aed3b-a142-4431-9a67-330458b1f44b",
   "metadata": {},
   "source": [
    "Get the paths of the images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9d80e-2ab1-4b2a-9d07-206faafa2daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_paths = [os.path.join(train_input_path, path) for path in os.listdir(train_input_path) if path.endswith(\".tif\")]\n",
    "train_mask_paths = [os.path.join(train_mask_path, path) for path in os.listdir(train_mask_path) if path.endswith(\".tif\")]\n",
    "print(\"Input images: \" + str(len(train_input_paths)))\n",
    "print(\"Input masks: \" + str(len(train_mask_paths)))\n",
    "print(\"---\")\n",
    "test_input_paths = [os.path.join(test_input_path, path) for path in os.listdir(test_input_path) if path.endswith(\".tif\")]\n",
    "test_mask_paths = [os.path.join(test_mask_path, path) for path in os.listdir(test_mask_path) if path.endswith(\".tif\")]\n",
    "print(\"Test images: \" + str(len(test_input_paths)))\n",
    "print(\"Test masks: \" + str(len(test_mask_paths)))\n",
    "train_path_dataset = tf.data.Dataset.from_tensor_slices((train_input_paths, train_mask_paths))\n",
    "test_path_dataset = tf.data.Dataset.from_tensor_slices((test_input_paths, test_mask_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642c138-51c5-4125-890a-09ebf07c3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in train_path_dataset.take(1):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c973b4-f9c4-42ba-bf76-aa7bed28d317",
   "metadata": {},
   "source": [
    "We define a function to read image/mask pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c6414-bbdf-4491-bf52-c38b2f412209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(img_path, segmentation_mask_path):\n",
    "    img_data = tf.io.read_file(img_path)\n",
    "    img = tfio.experimental.image.decode_tiff(img_data)\n",
    "    \n",
    "    segm_data = tf.io.read_file(segmentation_mask_path)\n",
    "    segm_mask = tfio.experimental.image.decode_tiff(segm_data)\n",
    "    \n",
    "    return img, segm_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2ecf9-1ddc-4f43-8283-88fab918601b",
   "metadata": {},
   "source": [
    "Normalize images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cccf0ff-0bb1-4287-b274-a9f6cc092da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(img, semg_mask):\n",
    "    img = tf.image.convert_image_dtype(img, \"float32\")\n",
    "    semg_mask = semg_mask / 255\n",
    "    return img, semg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e97685-325c-44d9-bce9-1046ce06fbbb",
   "metadata": {},
   "source": [
    "We create a dataset containing pairs of images/masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887e5ed-1eec-4cd9-ae24-7366da4619d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_path_dataset.map(read_images, num_parallel_calls=tf.data.AUTOTUNE).map(prepare_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_path_dataset.map(read_images, num_parallel_calls=tf.data.AUTOTUNE).map(prepare_images, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6a076-2650-4f6e-86e0-5f1f2101e618",
   "metadata": {},
   "source": [
    "Build train and validation batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c834af-7965-4cc7-8f31-03d3e065a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "VALIDATION_SIZE = int(round((len(train_dataset) * 20) / 100))\n",
    "print(\"validation data size: \" + str(VALIDATION_SIZE))\n",
    "print(\"train data size: \" + str(len(train_dataset) - VALIDATION_SIZE))\n",
    "validation_batches = train_dataset.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
    "train_batches = train_dataset.skip(VALIDATION_SIZE)\n",
    "train_batches = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_batches = train_batches.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad3b9a-6f15-436c-a8ad-707fc1689cfe",
   "metadata": {},
   "source": [
    "Display some random examples of pairs of input tiles and mask tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4e296-a1df-4640-a43b-f17c15da8457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "N = 3\n",
    "for image, mask in train_dataset.shuffle(len(train_dataset)).take(N):\n",
    "    print(image.shape)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(image) \n",
    "    ax2.imshow(mask)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ee497-b804-4471-9a34-ad109ea324af",
   "metadata": {},
   "source": [
    "Building blocks for the UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9bd22-5d7b-40d2-9890-ddbd79fdef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_block(x, n_filters):\n",
    "   # Conv2D then ReLU activation\n",
    "   x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "   # Conv2D then ReLU activation\n",
    "   x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "   return x\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "   f = double_conv_block(x, n_filters)\n",
    "   p = layers.MaxPool2D(2)(f)\n",
    "   p = layers.Dropout(0.3)(p)\n",
    "   return f, p\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "   # upsample\n",
    "   x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
    "   # concatenate\n",
    "   x = layers.concatenate([x, conv_features])\n",
    "   # dropout\n",
    "   x = layers.Dropout(0.3)(x)\n",
    "   # Conv2D twice with ReLU activation\n",
    "   x = double_conv_block(x, n_filters)\n",
    "   return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07788968-fff5-4c06-bce2-6f6281cc917c",
   "metadata": {},
   "source": [
    "Function that builds the UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3bb9e-6e68-429f-adb4-fe3897f9517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_model():\n",
    "   inputs = layers.Input(shape=(256,256,3))\n",
    "   # encoder: contracting path - downsample\n",
    "   # 1 - downsample\n",
    "   f1, p1 = downsample_block(inputs, 64)\n",
    "   # 2 - downsample\n",
    "   f2, p2 = downsample_block(p1, 128)\n",
    "   # 3 - downsample\n",
    "   f3, p3 = downsample_block(p2, 256)\n",
    "   # 4 - downsample\n",
    "   f4, p4 = downsample_block(p3, 512)\n",
    "   # 5 - bottleneck\n",
    "   bottleneck = double_conv_block(p4, 1024)\n",
    "   # decoder: expanding path - upsample\n",
    "   # 6 - upsample\n",
    "   u6 = upsample_block(bottleneck, f4, 512)\n",
    "   # 7 - upsample\n",
    "   u7 = upsample_block(u6, f3, 256)\n",
    "   # 8 - upsample\n",
    "   u8 = upsample_block(u7, f2, 128)\n",
    "   # 9 - upsample\n",
    "   u9 = upsample_block(u8, f1, 64)\n",
    "   # outputs\n",
    "   outputs = layers.Conv2D(2, 1, padding=\"same\", activation = \"softmax\")(u9)\n",
    "   # unet model with Keras Functional API\n",
    "   unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "   return unet_model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0225aac-60da-468c-b5c0-b44dc3db466a",
   "metadata": {},
   "source": [
    "Build the UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dac285-df6d-4579-ac9b-6df1114aa0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = build_unet_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32511d1-36df-45dd-a256-30d4d8813826",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(unet_model, show_shapes=True)\n",
    "\"model.png written\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983f8cb-6796-41af-bbbc-69332629a658",
   "metadata": {},
   "source": [
    "Compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a5cfc-accb-4dd4-a5b4-5ae111f1aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c214323a-a33b-479e-a303-024e8b91d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "STEPS_PER_EPOCH = len(train_dataset) // BATCH_SIZE\n",
    "VAL_SUBSPLITS = 5\n",
    "VAL_LENGTH = VALIDATION_SIZE\n",
    "VALIDATION_STEPS = VAL_LENGTH // BATCH_SIZE // VAL_SUBSPLITS\n",
    "model_history = unet_model.fit(train_batches,\n",
    "                              epochs=NUM_EPOCHS,\n",
    "                              steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                              validation_steps=VALIDATION_STEPS,\n",
    "                              validation_data=validation_batches\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c49187-4ed2-4013-bf96-0baf9cbed6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
